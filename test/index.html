<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Face Tracking Test</title>
    <style>
        body { font-family: sans-serif; }
        #status { margin-bottom: 20px; font-weight: bold; }
    </style>
</head>
<body>
    <h1>Face Tracking Test</h1>
    <p>This page tests the logic used in the extension. Note: The extension uses bundle, this is a standalone test.</p>
    <div id="status">Loading...</div>

    <!-- We need to shim chrome.runtime.getURL for the logic to work unmodified if possible, 
         or just replicate logic here. 
         Since the content script relies on specific paths and imports, we can't just drop it here without build.
         So we will just use a simple script that mimics the logic to prove it works.
    -->
    <script type="module">
        import { FilesetResolver, FaceLandmarker } from 'https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/+esm';

        const status = document.getElementById('status');
        let faceLandmarker;
        let video;
        let canvas;
        let runningMode = "VIDEO";

        async function run() {
            status.textContent = "Loading model...";
            const filesetResolver = await FilesetResolver.forVisionTasks(
                "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm"
            );
            faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {
                baseOptions: {
                    modelAssetPath: "https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task",
                    delegate: "GPU"
                },
                outputFaceBlendshapes: true,
                runningMode: runningMode,
                numFaces: 1
            });
            status.textContent = "Model loaded. Starting camera...";
            startCamera();
        }

        function startCamera() {
            video = document.createElement('video');
            video.autoplay = true;
            video.playsInline = true;
            document.body.appendChild(video);

            navigator.mediaDevices.getUserMedia({ video: true }).then((stream) => {
                video.srcObject = stream;
                video.addEventListener("loadeddata", predictWebcam);
                status.textContent = "Camera running. tracking faces...";
            });
        }

        let lastVideoTime = -1;
        let results = undefined;
        let currentExpression = 'neutral';

        async function predictWebcam() {
            if (lastVideoTime !== video.currentTime) {
                lastVideoTime = video.currentTime;
                results = faceLandmarker.detectForVideo(video, performance.now());
            }

            if (results.faceBlendshapes && results.faceBlendshapes.length > 0 && results.faceBlendshapes[0].categories) {
                const output = results.faceBlendshapes[0].categories;
                detectExpression(output);
            }

            window.requestAnimationFrame(predictWebcam);
        }

        function detectExpression(blendshapes) {
            const smileLeft = blendshapes.find(c => c.categoryName === 'mouthSmileLeft');
            const smileRight = blendshapes.find(c => c.categoryName === 'mouthSmileRight');
            
            if (smileLeft && smileRight) {
                const smileScore = (smileLeft.score + smileRight.score) / 2;
                status.textContent = `Expression: ${currentExpression} (Score: ${smileScore.toFixed(2)})`;
                
                if (smileScore > 0.6) {
                    currentExpression = 'smile ðŸ˜Š';
                } else if (smileScore < 0.3) {
                    currentExpression = 'neutral ðŸ˜';
                }
            }
        }

        run();
    </script>
</body>
</html>
